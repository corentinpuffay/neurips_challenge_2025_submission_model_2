Combination 11
Settings: Settings(batch_size=2048, accumulation_steps=4, d_model=16, dim_feedforward=32, num_layers=1, nheads=1, dropout=0.3, huber_delta=2.0, n_epochs=100, loss_fn='NRMSE')
Correlation between min loss and test loss: 0.7859
Correlation between max loss and test loss: 0.7764
Correlation between validation loss and test loss: -0.3408
Test loss at epoch 26 with lowest validation loss (0.9921): 0.9968
Epoch	Min Loss	Max Loss	Val Loss	Test Loss	Learning Rate
0	1.0281	1.6380	1.0122	1.0294	0.010000
1	1.0045	1.0584	1.0218	1.0029	0.010000
2	0.9989	1.0243	1.0016	1.0004	0.010000
3	0.9922	1.0075	1.0072	0.999	0.010000
4	0.9918	1.0061	1.0033	0.999	0.010000
5	0.9888	1.0002	1.0038	0.9984	0.010000
6	0.9873	0.9978	1.0027	0.9979	0.010000
7	0.9875	0.9976	1.001	0.9974	0.010000
8	0.9835	0.9962	0.9998	0.9971	0.010000
9	0.9800	0.9953	0.9995	0.9967	0.010000
10	0.9833	0.9980	0.9985	0.9967	0.010000
11	0.9804	0.9966	0.9976	0.9974	0.010000
12	0.9800	0.9964	0.9977	0.9974	0.010000
13	0.9794	0.9957	0.9981	0.997	0.010000
14	0.9779	0.9956	0.9979	0.9974	0.010000
15	0.9749	0.9937	0.9983	0.9973	0.010000
16	0.9778	0.9938	0.9978	0.9985	0.010000
17	0.9755	0.9912	0.9981	0.998	0.010000
18	0.9720	0.9917	0.9977	0.9981	0.010000
19	0.9750	0.9926	0.9969	0.9985	0.010000
20	0.9719	0.9931	0.9983	0.9983	0.010000
21	0.9743	0.9909	0.9986	0.9979	0.010000
22	0.9731	0.9916	0.9966	1.0004	0.010000
23	0.9739	0.9906	0.9977	0.9981	0.010000
24	0.9707	0.9881	0.9981	0.9983	0.010000
25	0.9727	1.3380	1.0266	1.0012	0.010000
26	0.9753	1.0066	0.9921	0.9968	0.010000
27	0.9689	0.9902	1.0145	0.9957	0.010000
28	0.9676	0.9857	1.0065	0.9935	0.010000
29	0.9671	0.9847	1.0059	0.9948	0.010000
30	0.9663	0.9854	1.007	0.9942	0.010000
31	0.9653	0.9837	1.0095	0.9945	0.010000
32	0.9626	0.9843	1.0084	0.9935	0.010000
33	0.9620	0.9841	1.0059	0.9944	0.010000
34	0.9639	0.9831	1.006	0.9943	0.010000
35	0.9628	0.9837	1.0056	0.9939	0.010000
36	0.9611	0.9843	1.0072	0.9946	0.010000
37	0.9620	0.9823	1.0041	0.9938	0.010000
38	0.9595	0.9813	1.0052	0.9943	0.010000
39	0.9607	0.9791	1.0108	0.995	0.010000
40	0.9591	0.9828	1.0061	0.9939	0.010000
41	0.9602	0.9812	1.0055	0.9947	0.010000
42	0.9598	0.9799	1.006	0.9943	0.010000
43	0.9586	0.9821	1.0043	0.9936	0.010000
44	0.9594	0.9832	1.0084	0.9946	0.010000
45	0.9595	0.9828	1.0037	0.9939	0.010000
46	0.9574	0.9833	1.0052	0.9935	0.010000
47	0.9580	0.9798	1.01	0.9945	0.010000
48	0.9585	0.9810	1.0015	0.9943	0.010000
49	0.9553	0.9823	1.0114	0.994	0.010000

Combination 35
Settings: Settings(batch_size=2048, accumulation_steps=4, d_model=16, dim_feedforward=32, num_layers=3, nheads=1, dropout=0.3, huber_delta=2.0, n_epochs=100, loss_fn='NRMSE')
Correlation between min loss and test loss: 0.5323
Correlation between max loss and test loss: 0.5920
Correlation between validation loss and test loss: 0.6279
Test loss at epoch 25 with lowest validation loss (0.9990): 1.0136
Epoch	Min Loss	Max Loss	Val Loss	Test Loss	Learning Rate
0	1.0132	1.6070	1.0141	0.9977	0.010000
1	1.0074	1.0845	0.9999	1.0048	0.010000
2	1.0029	1.0175	1.0188	1.0008	0.010000
3	0.9957	1.0124	1.0052	0.9983	0.010000
4	0.9950	1.0060	1.0075	0.9972	0.010000
5	0.9908	1.0009	1.0081	0.996	0.010000
6	0.9863	0.9963	1.0126	0.9967	0.010000
7	0.9819	0.9976	1.0068	0.995	0.010000
8	0.9796	0.9957	1.0074	0.9956	0.010000
9	0.9714	0.9922	1.0036	0.997	0.010000
10	0.9721	0.9926	1.0061	0.996	0.010000
11	0.9710	0.9897	1.004	0.9953	0.010000
12	0.9693	0.9890	1.0104	0.9966	0.010000
13	0.9693	0.9878	1.0077	0.9954	0.010000
14	0.9691	0.9890	1.0076	0.9946	0.010000
15	0.9673	0.9878	1.0062	0.9952	0.010000
16	0.9664	0.9860	1.006	0.9942	0.010000
17	0.9647	0.9873	1.0051	0.9959	0.010000
18	0.9681	0.9838	1.011	0.9977	0.010000
19	0.9621	0.9859	1.0107	0.9958	0.010000
20	0.9634	0.9873	1.0097	0.9957	0.010000
21	0.9644	0.9840	1.0162	0.9966	0.010000
22	0.9614	0.9836	1.0203	0.9965	0.010000
23	0.9629	0.9895	1.0013	0.9946	0.010000
24	0.9639	0.9900	1.0307	1.0029	0.010000
25	0.9755	2.4167	0.999	1.0136	0.010000
26	0.9968	1.0198	1.02	1.0018	0.010000
27	0.9945	1.0041	1.0227	1.0027	0.010000
28	0.9905	1.0015	1.0115	0.9978	0.010000
29	0.9774	0.9965	1.0025	0.9937	0.001000
30	0.9760	0.9905	0.9998	0.9931	0.001000
31	0.9729	0.9900	0.9996	0.993	0.001000
32	0.9730	0.9898	1.0003	0.9928	0.001000
33	0.9718	0.9891	1.0002	0.9927	0.001000
34	0.9666	0.9886	1.0008	0.9925	0.001000
35	0.9670	0.9859	1.0011	0.9925	0.001000
36	0.9691	0.9842	1.0019	0.9925	0.001000
37	0.9662	0.9867	1.0018	0.9925	0.001000
38	0.9652	0.9858	1.0018	0.9926	0.001000
39	0.9620	0.9878	1.0016	0.9925	0.001000
40	0.9635	0.9846	1.0024	0.9925	0.001000
41	0.9634	0.9847	1.0031	0.9926	0.001000
42	0.9613	0.9860	1.0041	0.9927	0.001000
43	0.9621	0.9842	1.0035	0.993	0.001000
44	0.9637	0.9838	1.0036	0.993	0.001000
45	0.9603	0.9852	1.0036	0.9932	0.001000
46	0.9641	0.9851	1.0043	0.9932	0.001000

